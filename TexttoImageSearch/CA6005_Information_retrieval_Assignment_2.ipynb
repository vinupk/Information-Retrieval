{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-GBPEcNz6ym2BZazCgARibl979zj-mf0",
      "authorship_tag": "ABX9TyMucf05AYJKAcPouQMTc4Zl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import csv\n",
        "import cv2\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "Ji6-XWFtwFRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518b2740-e326-4f55-c56f-004f3fc7c4ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that reads image dataset\n",
        "def build_image_index(csv_file):\n",
        "    image_index = {}\n",
        "    ids = []\n",
        "    with open(csv_file, 'r') as file:\n",
        "        csv_reader = csv.reader(file)\n",
        "        for row in csv_reader:\n",
        "            image_id, path, label = row\n",
        "            image_index[image_id] = {'path': path, 'label': label}\n",
        "            ids.append(image_id)\n",
        "    return image_index, ids\n"
      ],
      "metadata": {
        "id": "PfrlYXDEGaiJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # Step 1: Normalize to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 3: Remove punctuation from each word\n",
        "    punc = str.maketrans('', '', string.punctuation)\n",
        "    non_punc = [w.translate(punc) for w in tokens]\n",
        "\n",
        "    # Step 4: Remove non-alphabetic tokens\n",
        "    words = [word for word in non_punc if word.isalpha()]\n",
        "\n",
        "    # Step 5: Remove stop words from tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "\n",
        "    # Step 6: Stemming of tokens\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_text = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    return ' '.join(map(str, stemmed_text))"
      ],
      "metadata": {
        "id": "RvykAFFXl_6b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess image, like resize to target image, desaturate, etc\n",
        "def preprocess_image(image_path, target_size=(256, 256)):\n",
        "    # Read the image from the given path\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Resize the image to the target size\n",
        "    image = cv2.resize(image, target_size)\n",
        "    return image"
      ],
      "metadata": {
        "id": "L2gCK93OVdNy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns text surrogate\n",
        "def extract_text_surrogate(text):\n",
        "    # Use a text feature extractor (e.g., Bag of Words, TF-IDF) to convert the text into a vector representation (surrogate)\n",
        "    vectorizer = CountVectorizer()\n",
        "    text_matrix = vectorizer.fit_transform([text])\n",
        "    surrogate = text_matrix.toarray()\n",
        "    return surrogate"
      ],
      "metadata": {
        "id": "BTj6YOywFm3e"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns text surrogate\n",
        "def extract_text_surrogate(text):\n",
        "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "    text_surrogate = vectorizer.fit_transform([text])\n",
        "    return text_surrogate"
      ],
      "metadata": {
        "id": "aMyQJiHOIKgS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that search images for a given text\n",
        "def search_images_by_text(text_query, image_index, image_ids):\n",
        "  # Preprocess the text query\n",
        "  processed_text_query = preprocess_text(text_query)\n",
        "\n",
        "  # Extract the text labels for all images\n",
        "  image_labels = [image_index[image_id]['label'] for image_id in image_ids]\n",
        "\n",
        "  # Initialize the TfidfVectorizer\n",
        "  vectorizer = TfidfVectorizer(tokenizer=preprocess_text)\n",
        "\n",
        "  # Fit and transform the image labels to get their text surrogates\n",
        "  text_surrogates = vectorizer.fit_transform(image_labels)\n",
        "\n",
        "  # Transform the text query to get its text surrogate\n",
        "  text_query_surrogate = vectorizer.transform([text_query])\n",
        "\n",
        "  similarities = {}\n",
        "  for i, image_id in enumerate(image_ids):\n",
        "    # Calculate the similarity between the query's text surrogate and the image's text surrogate\n",
        "    similarity = cosine_similarity(text_query_surrogate, text_surrogates[i])\n",
        "    similarities[image_id] = similarity[0][0]\n",
        "\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "  top_similarities = sorted_similarities[:5]\n",
        "\n",
        "  return top_similarities"
      ],
      "metadata": {
        "id": "T5j75Q2izz-z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file containing the labeled image data\n",
        "csv_file = '/content/drive/MyDrive/image_search/reverse_image_search.csv'\n",
        "image_index, ids = build_image_index(csv_file)\n",
        "\n",
        "# Perform a text-based image search\n",
        "text_query = \"fish\"\n",
        "search_results = search_images_by_text(text_query, image_index, ids)\n",
        "\n",
        "# Print the search results\n",
        "print(\"Top 5 Similar Images:\")\n",
        "print(\"+------------+--------------+-------------------+\")\n",
        "print(\"|  Image ID  | Similarity  |     Image Path    |\")\n",
        "print(\"+------------+--------------+-------------------+\")\n",
        "for image_id, similarity in search_results:\n",
        "  image_path = image_index[image_id]['path']\n",
        "  print(f\"|  {image_id}  |   {similarity:.4f}    |   {image_path}   |\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP0QhhLE0P7b",
        "outputId": "f593d249-859f-4d5f-b54e-1147fe639d82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Similar Images:\n",
            "+------------+--------------+-------------------+\n",
            "|  Image ID  | Similarity  |     Image Path    |\n",
            "+------------+--------------+-------------------+\n",
            "|  640  |   0.7200    |   /content/drive/MyDrive/image_search/train/goldfish/n01443537_1415.JPEG   |\n",
            "|  641  |   0.7200    |   /content/drive/MyDrive/image_search/train/goldfish/n01443537_2637.JPEG   |\n",
            "|  642  |   0.7200    |   /content/drive/MyDrive/image_search/train/goldfish/n01443537_19638.JPEG   |\n",
            "|  643  |   0.7200    |   /content/drive/MyDrive/image_search/train/goldfish/n01443537_2819.JPEG   |\n",
            "|  644  |   0.7200    |   /content/drive/MyDrive/image_search/train/goldfish/n01443537_13189.JPEG   |\n"
          ]
        }
      ]
    }
  ]
}